{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Auto-Keras is an open source software library for automated machine learning (AutoML). It is developed by DATA Lab at Texas A&M University and community contributors. The ultimate goal of AutoML is to provide easily accessible deep learning tools to domain experts with limited data science or machine learning background. Auto-Keras provides functions to automatically search for architecture and hyperparameters of deep learning models. AutoKeras 1.0 is coming soon! Installation To install the package, please use the pip installation as follows: pip3 install autokeras # for 0.4 version pip3 install git+git://github.com/keras-team/autokeras@master#egg=autokeras # for 1.0 version Note: currently, Auto-Keras is only compatible with: Python 3.6 . Example Here is a short example of using the package. import autokeras as ak clf = ak.ImageClassifier() clf.fit(x_train, y_train) results = clf.predict(x_test) For detailed tutorial, please check here . Cite this work Haifeng Jin, Qingquan Song, and Xia Hu. \"Auto-keras: An efficient neural architecture search system.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019. ( Download ) Biblatex entry: @inproceedings{jin2019auto, title={Auto-Keras: An Efficient Neural Architecture Search System}, author={Jin, Haifeng and Song, Qingquan and Hu, Xia}, booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining}, pages={1946--1956}, year={2019}, organization={ACM} } Community You can use Gitter to communicate with people who are also interested in Auto-Keras. You can also follow us on Twitter @autokeras for the latest news. Contributing Code You can follow the Contributing Guide for details. The easist way to contribute is to resolve the issues with the \" call for contributors \" tag. They are friendly to beginners. Support Auto-Keras We accept donations on Open Collective . Thank every backer for supporting us! DISCLAIMER Please note that this is a pre-release version of the Auto-Keras which is still undergoing final testing before its official release. The website, its software and all content found on it are provided on an \u201cas is\u201d and \u201cas available\u201d basis. Auto-Keras does not give any warranties, whether express or implied, as to the suitability or usability of the website, its software or any of its content. Auto-Keras will not be liable for any loss, whether such loss is direct, indirect, special or consequential, suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user\u2019s own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs, glitches, lack of functionality or other problems on the website, please let us know immediately so we can rectify these accordingly. Your help in this regard is greatly appreciated. Acknowledgements The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M.","title":"Home"},{"location":"#autokeras-10-is-coming-soon","text":"","title":"AutoKeras 1.0 is coming soon!"},{"location":"#installation","text":"To install the package, please use the pip installation as follows: pip3 install autokeras # for 0.4 version pip3 install git+git://github.com/keras-team/autokeras@master#egg=autokeras # for 1.0 version Note: currently, Auto-Keras is only compatible with: Python 3.6 .","title":"Installation"},{"location":"#example","text":"Here is a short example of using the package. import autokeras as ak clf = ak.ImageClassifier() clf.fit(x_train, y_train) results = clf.predict(x_test) For detailed tutorial, please check here .","title":"Example"},{"location":"#cite-this-work","text":"Haifeng Jin, Qingquan Song, and Xia Hu. \"Auto-keras: An efficient neural architecture search system.\" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019. ( Download ) Biblatex entry: @inproceedings{jin2019auto, title={Auto-Keras: An Efficient Neural Architecture Search System}, author={Jin, Haifeng and Song, Qingquan and Hu, Xia}, booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining}, pages={1946--1956}, year={2019}, organization={ACM} }","title":"Cite this work"},{"location":"#community","text":"You can use Gitter to communicate with people who are also interested in Auto-Keras. You can also follow us on Twitter @autokeras for the latest news.","title":"Community"},{"location":"#contributing-code","text":"You can follow the Contributing Guide for details. The easist way to contribute is to resolve the issues with the \" call for contributors \" tag. They are friendly to beginners.","title":"Contributing Code"},{"location":"#support-auto-keras","text":"We accept donations on Open Collective . Thank every backer for supporting us!","title":"Support Auto-Keras"},{"location":"#disclaimer","text":"Please note that this is a pre-release version of the Auto-Keras which is still undergoing final testing before its official release. The website, its software and all content found on it are provided on an \u201cas is\u201d and \u201cas available\u201d basis. Auto-Keras does not give any warranties, whether express or implied, as to the suitability or usability of the website, its software or any of its content. Auto-Keras will not be liable for any loss, whether such loss is direct, indirect, special or consequential, suffered by any party as a result of their use of the libraries or content. Any usage of the libraries is done at the user\u2019s own risk and the user will be solely responsible for any damage to any computer system or loss of data that results from such activities. Should you encounter any bugs, glitches, lack of functionality or other problems on the website, please let us know immediately so we can rectify these accordingly. Your help in this regard is greatly appreciated.","title":"DISCLAIMER"},{"location":"#acknowledgements","text":"The authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M.","title":"Acknowledgements"},{"location":"about/","text":"About This package is developed by DATA LAB at Texas A&M University and community contributors. Main Contributors Haifeng Jin : Designed and developed the overall framework. Implemented Bayesian optimization and network morphism. Develop the pytorch backend. Qingquan Song : Developed the keras backend. Implemented the tabular data classification and regression module. Wrote the getting started tutorial. Tsung-Lin Yang : Implemented ResNet and DenseNet Generator. Support Google Colab, and Multi-GPU training. Refactored CnnModule interface. Added accurate Timeout control support. Yashwanth Reddy : Created MLP and CNN module. Created pretrained model for face detection. Satya Kesav : Supported multiple dimensions of image (e.g. 1D, 3D Convolution) and processing datasets having arbitrary image sizes Developed the BERT based Natural Language API for AutoKeras, including two pretrained models of sentiment analysis and topic classifier. and generic modules for text classification and regression. Xia \"Ben\" Hu : Project lead and maintainer.","title":"About"},{"location":"about/#about","text":"This package is developed by DATA LAB at Texas A&M University and community contributors.","title":"About"},{"location":"about/#main-contributors","text":"Haifeng Jin : Designed and developed the overall framework. Implemented Bayesian optimization and network morphism. Develop the pytorch backend. Qingquan Song : Developed the keras backend. Implemented the tabular data classification and regression module. Wrote the getting started tutorial. Tsung-Lin Yang : Implemented ResNet and DenseNet Generator. Support Google Colab, and Multi-GPU training. Refactored CnnModule interface. Added accurate Timeout control support. Yashwanth Reddy : Created MLP and CNN module. Created pretrained model for face detection. Satya Kesav : Supported multiple dimensions of image (e.g. 1D, 3D Convolution) and processing datasets having arbitrary image sizes Developed the BERT based Natural Language API for AutoKeras, including two pretrained models of sentiment analysis and topic classifier. and generic modules for text classification and regression. Xia \"Ben\" Hu : Project lead and maintainer.","title":"Main Contributors"},{"location":"auto_model/","text":"[source] AutoModel class autokeras.auto_model.AutoModel(inputs, outputs, name='auto_model', max_trials=100, directory=None, seed=None) A Model defined by inputs and outputs. AutoModel combines a HyperModel and a Tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has fit() and predict() methods. The user can specify the inputs and outputs of the AutoModel. It will infer the rest of the high-level neural architecture. Arguments inputs : A list of or a HyperNode instance. The input node(s) of the AutoModel. outputs : A list of or a HyperHead instance. The output head(s) of the AutoModel. name : String. The name of the AutoModel. Defaults to 'auto_model'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed. AutoModel methods fit fit(x=None, y=None, validation_split=0, validation_data=None) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. y : numpy.ndarray or tensorflow.Dataset. Training data y. validation_split : Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy arrays or tensors tuple (x_val, y_val, val_sample_weights) of Numpy arrays dataset or a dataset iterator For the first two cases, batch_size must be provided. For the last case, validation_steps must be provided. **kwargs : Any arguments supported by keras.Model.fit. predict predict(x, batch_size=32) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.","title":"AutoModel"},{"location":"auto_model/#automodel-class","text":"autokeras.auto_model.AutoModel(inputs, outputs, name='auto_model', max_trials=100, directory=None, seed=None) A Model defined by inputs and outputs. AutoModel combines a HyperModel and a Tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has fit() and predict() methods. The user can specify the inputs and outputs of the AutoModel. It will infer the rest of the high-level neural architecture. Arguments inputs : A list of or a HyperNode instance. The input node(s) of the AutoModel. outputs : A list of or a HyperHead instance. The output head(s) of the AutoModel. name : String. The name of the AutoModel. Defaults to 'auto_model'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed.","title":"AutoModel class"},{"location":"auto_model/#automodel-methods","text":"","title":"AutoModel methods"},{"location":"auto_model/#fit","text":"fit(x=None, y=None, validation_split=0, validation_data=None) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. y : numpy.ndarray or tensorflow.Dataset. Training data y. validation_split : Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy arrays or tensors tuple (x_val, y_val, val_sample_weights) of Numpy arrays dataset or a dataset iterator For the first two cases, batch_size must be provided. For the last case, validation_steps must be provided. **kwargs : Any arguments supported by keras.Model.fit.","title":"fit"},{"location":"auto_model/#predict","text":"predict(x, batch_size=32) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.","title":"predict"},{"location":"block/","text":"[source] ImageBlock autokeras.hypermodel.hyperblock.ImageBlock(block_type=None, normalize=True, augment=True, seed=None) Block for image data. The image blocks is a block choosing from ResNetBlock, XceptionBlock, ConvBlock, which is controlled by a hyperparameter, 'block_type'. Arguments block_type : String. 'resnet', 'xception', 'vanilla'. The type of HyperBlock to use. If unspecified, it will be tuned automatically. normalize : Boolean. Whether to channel-wise normalize the images. If unspecified, it will be tuned automatically. augment : Boolean. Whether to do image augmentation. If unspecified, it will be tuned automatically. [source] TextBlock autokeras.hypermodel.hyperblock.TextBlock(vectorizer=None, pretraining=None) Block for text data. Arguments vectorizer : String. 'sequence' or 'ngram'. If it is 'sequence', TextToIntSequence will be used. If it is 'ngram', TextToNgramVector will be used. If unspecified, it will be tuned automatically. pretraining : Boolean. Whether to use pretraining weights in the N-gram vectorizer. If unspecified, it will be tuned automatically. [source] StructuredDataBlock autokeras.hypermodel.hyperblock.StructuredDataBlock(feature_engineering=True, module_type=None) Block for structured data. Arguments feature_engineering : Boolean. Whether to use feature engineering block. Defaults to True. If specified as None, it will be tuned automatically. module_type : String. 'dense' or 'lightgbm'. If it is 'dense', DenseBlock will be used. If it is 'lightgbm', LightGBMBlock will be used. If unspecified, it will be tuned automatically. [source] ResNetBlock autokeras.hypermodel.block.ResNetBlock(version=None, pooling=None) Block for ResNet. Arguments version : String. 'v1', 'v2' or 'next'. The type of ResNet to use. If left unspecified, it will be tuned automatically. pooling : String. 'avg', 'max'. The type of pooling layer to use. If left unspecified, it will be tuned automatically. [source] XceptionBlock autokeras.hypermodel.block.XceptionBlock(activation=None, initial_strides=None, num_residual_blocks=None, pooling=None) XceptionBlock. An Xception structure, used for specifying your model with specific datasets. The original Xception architecture is from https://arxiv.org/abs/1610.02357. The data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. This XceptionBlock returns a similar architecture as Xception except without the last (optional) fully connected layer(s) and logistic regression. The size of this architecture could be decided by HyperParameters , to get an architecture with a half, an identical, or a double size of the original one. Arguments activation : String. 'selu' or 'relu'. If left unspecified, it will be tuned automatically. initial_strides : Int. If left unspecified, it will be tuned automatically. num_residual_blocks : Int. If left unspecified, it will be tuned automatically. pooling : String. 'ave', 'flatten', or 'max'. If left unspecified, it will be tuned automatically. [source] ConvBlock autokeras.hypermodel.block.ConvBlock(kernel_size=None, num_blocks=None, separable=None) Block for vanilla ConvNets. Arguments kernel_size : Int. If left unspecified, it will be tuned automatically. num_blocks : Int. The number of conv blocks. If left unspecified, it will be tuned automatically. separable : Boolean. Whether to use separable conv layers. If left unspecified, it will be tuned automatically. [source] RNNBlock autokeras.hypermodel.block.RNNBlock(return_sequences=False, bidirectional=None, num_layers=None, layer_type=None) An RNN Block. Arguments return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. Defaults to False. bidirectional : Boolean. Bidirectional RNN. If left unspecified, it will be tuned automatically. num_layers : Int. The number of layers in RNN. If left unspecified, it will be tuned automatically. layer_type : String. 'gru' or 'lstm'. If left unspecified, it will be tuned automatically. [source] Merge autokeras.hypermodel.block.Merge(merge_type=None) Merge block to merge multiple nodes into one. Arguments merge_type : String. 'add' or 'concatenate'. If left unspecified, it will be tuned automatically.","title":"Block"},{"location":"block/#imageblock","text":"autokeras.hypermodel.hyperblock.ImageBlock(block_type=None, normalize=True, augment=True, seed=None) Block for image data. The image blocks is a block choosing from ResNetBlock, XceptionBlock, ConvBlock, which is controlled by a hyperparameter, 'block_type'. Arguments block_type : String. 'resnet', 'xception', 'vanilla'. The type of HyperBlock to use. If unspecified, it will be tuned automatically. normalize : Boolean. Whether to channel-wise normalize the images. If unspecified, it will be tuned automatically. augment : Boolean. Whether to do image augmentation. If unspecified, it will be tuned automatically. [source]","title":"ImageBlock"},{"location":"block/#textblock","text":"autokeras.hypermodel.hyperblock.TextBlock(vectorizer=None, pretraining=None) Block for text data. Arguments vectorizer : String. 'sequence' or 'ngram'. If it is 'sequence', TextToIntSequence will be used. If it is 'ngram', TextToNgramVector will be used. If unspecified, it will be tuned automatically. pretraining : Boolean. Whether to use pretraining weights in the N-gram vectorizer. If unspecified, it will be tuned automatically. [source]","title":"TextBlock"},{"location":"block/#structureddatablock","text":"autokeras.hypermodel.hyperblock.StructuredDataBlock(feature_engineering=True, module_type=None) Block for structured data. Arguments feature_engineering : Boolean. Whether to use feature engineering block. Defaults to True. If specified as None, it will be tuned automatically. module_type : String. 'dense' or 'lightgbm'. If it is 'dense', DenseBlock will be used. If it is 'lightgbm', LightGBMBlock will be used. If unspecified, it will be tuned automatically. [source]","title":"StructuredDataBlock"},{"location":"block/#resnetblock","text":"autokeras.hypermodel.block.ResNetBlock(version=None, pooling=None) Block for ResNet. Arguments version : String. 'v1', 'v2' or 'next'. The type of ResNet to use. If left unspecified, it will be tuned automatically. pooling : String. 'avg', 'max'. The type of pooling layer to use. If left unspecified, it will be tuned automatically. [source]","title":"ResNetBlock"},{"location":"block/#xceptionblock","text":"autokeras.hypermodel.block.XceptionBlock(activation=None, initial_strides=None, num_residual_blocks=None, pooling=None) XceptionBlock. An Xception structure, used for specifying your model with specific datasets. The original Xception architecture is from https://arxiv.org/abs/1610.02357. The data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. This XceptionBlock returns a similar architecture as Xception except without the last (optional) fully connected layer(s) and logistic regression. The size of this architecture could be decided by HyperParameters , to get an architecture with a half, an identical, or a double size of the original one. Arguments activation : String. 'selu' or 'relu'. If left unspecified, it will be tuned automatically. initial_strides : Int. If left unspecified, it will be tuned automatically. num_residual_blocks : Int. If left unspecified, it will be tuned automatically. pooling : String. 'ave', 'flatten', or 'max'. If left unspecified, it will be tuned automatically. [source]","title":"XceptionBlock"},{"location":"block/#convblock","text":"autokeras.hypermodel.block.ConvBlock(kernel_size=None, num_blocks=None, separable=None) Block for vanilla ConvNets. Arguments kernel_size : Int. If left unspecified, it will be tuned automatically. num_blocks : Int. The number of conv blocks. If left unspecified, it will be tuned automatically. separable : Boolean. Whether to use separable conv layers. If left unspecified, it will be tuned automatically. [source]","title":"ConvBlock"},{"location":"block/#rnnblock","text":"autokeras.hypermodel.block.RNNBlock(return_sequences=False, bidirectional=None, num_layers=None, layer_type=None) An RNN Block. Arguments return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. Defaults to False. bidirectional : Boolean. Bidirectional RNN. If left unspecified, it will be tuned automatically. num_layers : Int. The number of layers in RNN. If left unspecified, it will be tuned automatically. layer_type : String. 'gru' or 'lstm'. If left unspecified, it will be tuned automatically. [source]","title":"RNNBlock"},{"location":"block/#merge","text":"autokeras.hypermodel.block.Merge(merge_type=None) Merge block to merge multiple nodes into one. Arguments merge_type : String. 'add' or 'concatenate'. If left unspecified, it will be tuned automatically.","title":"Merge"},{"location":"contributing/","text":"Contributing Guide Contributions are welcome, and greatly appreciated! Every little bit helps, and credit will always be given. We recommend you to check our Developer Tools Guide to make the development process easier and standard. Notably, you can follow the tag of call for contributors in the issues. Those issues are designed for the external contributors to solve. The pull requests solving these issues are most likely to be merged. There are many ways to contribute to Auto-Keras, including submit feedback, fix bugs, implement features, and write documentation. The guide for each type of contribution is as follows. Submit Feedback The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels. Fix Bugs: You may look through the GitHub issues for bugs. Anything tagged with \"bug report\" is open to whoever wants to implement it. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , and Documentation Guide to ensure your merge request meet our requirements. Implement Features You may look through the GitHub issues for feature requests. Anything tagged with \"feature request\" is open to whoever wants to implement it. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , Documentation Guide , and Testing Guide to ensure your merge request meet our requirements. Write Documentation The documentation of Auto-Keras is either directly written into the Markdown files in mkdocs directory , or automatically extracted from the docstrings by executing the autogen.py . In the first situation, you only need to change the markdown file. In the second situation, you need to change the docstrings and execute autogen.py to update the Markdown files. Please follow the Pull Request Guide to submit your pull request. Please also read Documentation Guide to ensure your merge request meet our requirements. Pull Request Guide Before you submit a pull request, check that it meets these guidelines: Fork the repository. Create a new branch from the master branch. Give your new branch a meaningful name. Pull request from your new branch to the master branch of the original autokeras repo. Give your pull request a meaningful name. Include \"resolves #issue_number\" in the description of the pull request and briefly describe your contribution. Submit the pull request from the first day of your development (after your first commit) and prefix the title of the pull request with [WIP] . When the contribution is complete, make sure the pull request passed the CI tests. Change the [WIP] to [MRG] . Set the reviewer to @jhfjhfj1 . For the case of bug fixes, add new test cases which would fail before your bug fix. If you are a collaborator of the autokeras repo, you don't need to fork the repository. Just create a new branch directly. You also need to change the assignee to the reviewer when request for code review. The reviewer will change the assignee back to you when finished the review. The assignee always means who should push the progress of the pull request now. Code Style Guide This project tries to closely follow the official Python Style Guide detailed in PEP8 . The docstrings follow the Google Python Style Guide . Please follow these style guide closely, especially for the docstrings, which would be extracted automatically to generate the documentation. Documentation Guide: The documentation should be provided in two ways, docstring, tutorial, and readme file. We prefer the documentation to be as complete as possible. Docstring All the methods and classes may directly be called by the user need to be documented with docstrings. The docstrings should contain all the fields required by the Google Python Style Guide . Tutorial You only need to add tutorials to your code if you are contributing or updating a new task module, e.g. TextClassifier, VideoClassifier, or a new function could be directly called by the user. You can modify mkdocs/docs/start.md to add your tutorial. The code example of your new task module should be added to the examples directory. Readme File You only need to add tutorials to your code if you are contributing or updating a new task module, e.g. TextClassifier, VideoClassifier. The readme file should be named as README.md . It should be written in Markdown. The content should contain your name, affiliation, and any reference to the method you use. Testing Guide Pytest is used to write the unit tests of Auto-Keras. You should test your code by writing unit testing code in tests directory. The testing file name should be the .py file with a prefix of test_ in the corresponding directory, e.g., the name should be test_layers.py if the code of which is to test layer.py . The tests should be run in the root directory of the project by executing the cov.sh file. It would output the coverage information into a directory named htmlcov . Please make sure the code coverage percentage does not decrease after your contribution, otherwise, the code will not be merged. Developer Tools Guide We highly recommend you to use Pycharm and virtualenvwrapper . Pycharm Pycharm is the best IDE for large project development in Python. We recommend you inspect the code before you pull request to fix any error and warning suggested by the inspection. Virtualenvwrapper Virtualenvwrapper is a tool to build separated Python environment for each project. In this way, you can install a different version of Tensorflow, Pytorch, or any other package for each project. We recommend you to create a virtualenv for autokeras development with virtualenvwrapper, and only install the packages required by autokeras with the corresponding version. The virtualenv should be created based on Python 3.6 interpreter. Use pycharm to select the virtualenv as interpreter . Reusable Code Guide You may checkout this code review video to get familiar with the code structure. Other than the base classes you have to extend, there are some other classes you can extend. ModelTrainer autokeras.model_trainer.ModelTrainer is a class for training Pytorch models. If needed a new metric or loss function other than the ones we have, you can add your own to loss_function.py and metric.py . You can follow its documentation and this example to use it. Make sure your loss function, metric, Pytorch model, and Dataloader are compatible with each other. Main Contributor List We really appreciate all the contributions. To show our appreciation to those who contributed most, we would like to maintain a list of main contributors. To be in the list, you need to meet the following requirments. 1. Be on campus of Texas A&M University. 2. Constantly present in our meetings. 3. Constantly contribute code to our repository. 4. Keep the above for over 6 months.","title":"Contributing Guide"},{"location":"contributing/#contributing-guide","text":"Contributions are welcome, and greatly appreciated! Every little bit helps, and credit will always be given. We recommend you to check our Developer Tools Guide to make the development process easier and standard. Notably, you can follow the tag of call for contributors in the issues. Those issues are designed for the external contributors to solve. The pull requests solving these issues are most likely to be merged. There are many ways to contribute to Auto-Keras, including submit feedback, fix bugs, implement features, and write documentation. The guide for each type of contribution is as follows.","title":"Contributing Guide"},{"location":"contributing/#submit-feedback","text":"The feedback should be submitted by creating an issue at GitHub issues . Select the related template (bug report, feature request, or custom) and add the corresponding labels.","title":"Submit Feedback"},{"location":"contributing/#fix-bugs","text":"You may look through the GitHub issues for bugs. Anything tagged with \"bug report\" is open to whoever wants to implement it. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , and Documentation Guide to ensure your merge request meet our requirements.","title":"Fix Bugs:"},{"location":"contributing/#implement-features","text":"You may look through the GitHub issues for feature requests. Anything tagged with \"feature request\" is open to whoever wants to implement it. Please follow the Pull Request Guide to submit your pull request. Please also read Code Style Guide , Documentation Guide , and Testing Guide to ensure your merge request meet our requirements.","title":"Implement Features"},{"location":"contributing/#write-documentation","text":"The documentation of Auto-Keras is either directly written into the Markdown files in mkdocs directory , or automatically extracted from the docstrings by executing the autogen.py . In the first situation, you only need to change the markdown file. In the second situation, you need to change the docstrings and execute autogen.py to update the Markdown files. Please follow the Pull Request Guide to submit your pull request. Please also read Documentation Guide to ensure your merge request meet our requirements.","title":"Write Documentation"},{"location":"contributing/#pull-request-guide","text":"Before you submit a pull request, check that it meets these guidelines: Fork the repository. Create a new branch from the master branch. Give your new branch a meaningful name. Pull request from your new branch to the master branch of the original autokeras repo. Give your pull request a meaningful name. Include \"resolves #issue_number\" in the description of the pull request and briefly describe your contribution. Submit the pull request from the first day of your development (after your first commit) and prefix the title of the pull request with [WIP] . When the contribution is complete, make sure the pull request passed the CI tests. Change the [WIP] to [MRG] . Set the reviewer to @jhfjhfj1 . For the case of bug fixes, add new test cases which would fail before your bug fix. If you are a collaborator of the autokeras repo, you don't need to fork the repository. Just create a new branch directly. You also need to change the assignee to the reviewer when request for code review. The reviewer will change the assignee back to you when finished the review. The assignee always means who should push the progress of the pull request now.","title":"Pull Request Guide"},{"location":"contributing/#code-style-guide","text":"This project tries to closely follow the official Python Style Guide detailed in PEP8 . The docstrings follow the Google Python Style Guide . Please follow these style guide closely, especially for the docstrings, which would be extracted automatically to generate the documentation.","title":"Code Style Guide"},{"location":"contributing/#documentation-guide","text":"The documentation should be provided in two ways, docstring, tutorial, and readme file. We prefer the documentation to be as complete as possible.","title":"Documentation Guide:"},{"location":"contributing/#docstring","text":"All the methods and classes may directly be called by the user need to be documented with docstrings. The docstrings should contain all the fields required by the Google Python Style Guide .","title":"Docstring"},{"location":"contributing/#tutorial","text":"You only need to add tutorials to your code if you are contributing or updating a new task module, e.g. TextClassifier, VideoClassifier, or a new function could be directly called by the user. You can modify mkdocs/docs/start.md to add your tutorial. The code example of your new task module should be added to the examples directory.","title":"Tutorial"},{"location":"contributing/#readme-file","text":"You only need to add tutorials to your code if you are contributing or updating a new task module, e.g. TextClassifier, VideoClassifier. The readme file should be named as README.md . It should be written in Markdown. The content should contain your name, affiliation, and any reference to the method you use.","title":"Readme File"},{"location":"contributing/#testing-guide","text":"Pytest is used to write the unit tests of Auto-Keras. You should test your code by writing unit testing code in tests directory. The testing file name should be the .py file with a prefix of test_ in the corresponding directory, e.g., the name should be test_layers.py if the code of which is to test layer.py . The tests should be run in the root directory of the project by executing the cov.sh file. It would output the coverage information into a directory named htmlcov . Please make sure the code coverage percentage does not decrease after your contribution, otherwise, the code will not be merged.","title":"Testing Guide"},{"location":"contributing/#developer-tools-guide","text":"We highly recommend you to use Pycharm and virtualenvwrapper .","title":"Developer Tools Guide"},{"location":"contributing/#pycharm","text":"Pycharm is the best IDE for large project development in Python. We recommend you inspect the code before you pull request to fix any error and warning suggested by the inspection.","title":"Pycharm"},{"location":"contributing/#virtualenvwrapper","text":"Virtualenvwrapper is a tool to build separated Python environment for each project. In this way, you can install a different version of Tensorflow, Pytorch, or any other package for each project. We recommend you to create a virtualenv for autokeras development with virtualenvwrapper, and only install the packages required by autokeras with the corresponding version. The virtualenv should be created based on Python 3.6 interpreter. Use pycharm to select the virtualenv as interpreter .","title":"Virtualenvwrapper"},{"location":"contributing/#reusable-code-guide","text":"You may checkout this code review video to get familiar with the code structure. Other than the base classes you have to extend, there are some other classes you can extend.","title":"Reusable Code Guide"},{"location":"contributing/#modeltrainer","text":"autokeras.model_trainer.ModelTrainer is a class for training Pytorch models. If needed a new metric or loss function other than the ones we have, you can add your own to loss_function.py and metric.py . You can follow its documentation and this example to use it. Make sure your loss function, metric, Pytorch model, and Dataloader are compatible with each other.","title":"ModelTrainer"},{"location":"contributing/#main-contributor-list","text":"We really appreciate all the contributions. To show our appreciation to those who contributed most, we would like to maintain a list of main contributors. To be in the list, you need to meet the following requirments. 1. Be on campus of Texas A&M University. 2. Constantly present in our meetings. 3. Constantly contribute code to our repository. 4. Keep the above for over 6 months.","title":"Main Contributor List"},{"location":"docker/","text":"Auto-Keras Docker Download Auto-Keras Docker image The following command download Auto-Keras docker image to your machine. docker pull garawalid/autokeras:latest Image releases are tagged using the following format: Tag Description latest Auto-Keras image devel Auto-Keras image that tracks Github repository Start Auto-Keras Docker container docker run -it --shm-size 2G garawalid/autokeras /bin/bash In case you need more memory to run the container, change the value of shm-size . ( Docker run reference ) Run application : To run a local script file.py using Auto-Keras within the container, mount the host directory -v hostDir:/app . docker run -it -v hostDir:/app --shm-size 2G garawalid/autokeras python file.py Example : Let's download the mnist example and run it within the container. Download the example : curl https://raw.githubusercontent.com/keras-team/autokeras/master/examples/a_simple_example/mnist.py --output mnist.py Run the mnist example : docker run -it -v \"$(pwd)\":/app --shm-size 2G garawalid/autokeras python mnist.py","title":"Docker"},{"location":"docker/#auto-keras-docker","text":"","title":"Auto-Keras Docker"},{"location":"docker/#download-auto-keras-docker-image","text":"The following command download Auto-Keras docker image to your machine. docker pull garawalid/autokeras:latest Image releases are tagged using the following format: Tag Description latest Auto-Keras image devel Auto-Keras image that tracks Github repository","title":"Download Auto-Keras Docker image"},{"location":"docker/#start-auto-keras-docker-container","text":"docker run -it --shm-size 2G garawalid/autokeras /bin/bash In case you need more memory to run the container, change the value of shm-size . ( Docker run reference )","title":"Start Auto-Keras Docker container"},{"location":"docker/#run-application","text":"To run a local script file.py using Auto-Keras within the container, mount the host directory -v hostDir:/app . docker run -it -v hostDir:/app --shm-size 2G garawalid/autokeras python file.py","title":"Run application :"},{"location":"docker/#example","text":"Let's download the mnist example and run it within the container. Download the example : curl https://raw.githubusercontent.com/keras-team/autokeras/master/examples/a_simple_example/mnist.py --output mnist.py Run the mnist example : docker run -it -v \"$(pwd)\":/app --shm-size 2G garawalid/autokeras python mnist.py","title":"Example :"},{"location":"graph_auto_model/","text":"[source] GraphAutoModel class autokeras.auto_model.GraphAutoModel(inputs, outputs, name='graph_auto_model', max_trials=100, directory=None, seed=None) A HyperModel defined by a graph of HyperBlocks. GraphAutoModel is a subclass of HyperModel. Besides the HyperModel properties, it also has a tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has fit() and predict() methods. The user can specify the high-level neural architecture by connecting the HyperBlocks with the functional API, which is the same as the Keras functional API. Arguments inputs : A list of or a HyperNode instances. The input node(s) of the GraphAutoModel. outputs : A list of or a HyperNode instances. The output node(s) of the GraphAutoModel. name : String. The name of the AutoModel. Defaults to 'graph_auto_model'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed. GraphAutoModel methods fit fit(x=None, y=None, validation_split=0, validation_data=None) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. y : numpy.ndarray or tensorflow.Dataset. Training data y. validation_split : Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy arrays or tensors tuple (x_val, y_val, val_sample_weights) of Numpy arrays dataset or a dataset iterator For the first two cases, batch_size must be provided. For the last case, validation_steps must be provided. **kwargs : Any arguments supported by keras.Model.fit. predict predict(x, batch_size=32) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.","title":"GraphAutoModel"},{"location":"graph_auto_model/#graphautomodel-class","text":"autokeras.auto_model.GraphAutoModel(inputs, outputs, name='graph_auto_model', max_trials=100, directory=None, seed=None) A HyperModel defined by a graph of HyperBlocks. GraphAutoModel is a subclass of HyperModel. Besides the HyperModel properties, it also has a tuner to tune the HyperModel. The user can use it in a similar way to a Keras model since it also has fit() and predict() methods. The user can specify the high-level neural architecture by connecting the HyperBlocks with the functional API, which is the same as the Keras functional API. Arguments inputs : A list of or a HyperNode instances. The input node(s) of the GraphAutoModel. outputs : A list of or a HyperNode instances. The output node(s) of the GraphAutoModel. name : String. The name of the AutoModel. Defaults to 'graph_auto_model'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed.","title":"GraphAutoModel class"},{"location":"graph_auto_model/#graphautomodel-methods","text":"","title":"GraphAutoModel methods"},{"location":"graph_auto_model/#fit","text":"fit(x=None, y=None, validation_split=0, validation_data=None) Search for the best model and hyperparameters for the AutoModel. It will search for the best model based on the performances on validation data. Arguments x : numpy.ndarray or tensorflow.Dataset. Training data x. y : numpy.ndarray or tensorflow.Dataset. Training data y. validation_split : Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. This argument is not supported when x is a dataset. validation_data : Data on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. validation_data will override validation_split . validation_data could be: tuple (x_val, y_val) of Numpy arrays or tensors tuple (x_val, y_val, val_sample_weights) of Numpy arrays dataset or a dataset iterator For the first two cases, batch_size must be provided. For the last case, validation_steps must be provided. **kwargs : Any arguments supported by keras.Model.fit.","title":"fit"},{"location":"graph_auto_model/#predict","text":"predict(x, batch_size=32) Predict the output for a given testing data. Arguments x : Any allowed types according to the input node. Testing data. batch_size : Int. Defaults to 32. **kwargs : Any arguments supported by keras.Model.predict. Returns A list of numpy.ndarray objects or a single numpy.ndarray. The predicted results.","title":"predict"},{"location":"head/","text":"[source] ClassificationHead autokeras.hypermodel.head.ClassificationHead(num_classes=None, multi_label=False, loss=None, metrics=None, dropout_rate=None) Classification Dense layers. Use sigmoid and binary crossentropy for binary classification and multi-label classification. Use softmax and categorical crossentropy for multi-class (more than 2) classification. Use Accuracy as metrics by default. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source] RegressionHead autokeras.hypermodel.head.RegressionHead(output_dim=None, loss=None, metrics=None, dropout_rate=None) Regression Dense layers. Use mean squared error as metrics and loss by default. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically.","title":"Head"},{"location":"head/#classificationhead","text":"autokeras.hypermodel.head.ClassificationHead(num_classes=None, multi_label=False, loss=None, metrics=None, dropout_rate=None) Classification Dense layers. Use sigmoid and binary crossentropy for binary classification and multi-label classification. Use softmax and categorical crossentropy for multi-class (more than 2) classification. Use Accuracy as metrics by default. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically. [source]","title":"ClassificationHead"},{"location":"head/#regressionhead","text":"autokeras.hypermodel.head.RegressionHead(output_dim=None, loss=None, metrics=None, dropout_rate=None) Regression Dense layers. Use mean squared error as metrics and loss by default. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. dropout_rate : Float. The dropout rate for the layers. If left unspecified, it will be tuned automatically.","title":"RegressionHead"},{"location":"nas/","text":"Neural Architecture Search To help the researchers to do experiments on neural architecture search (NAS), we have implemented several baseline methods using the Auto-Keras framework. The implementations are easy since only the core part of the search algorithm is needed. All other parts of NAS (e.g. data structures for storing neural architectures, training of the neural networks) are done by the Auto-Keras framework. Why implement NAS papers in Auto-Keras? The NAS papers usually evaluate their work with the same dataset (e.g. CIFAR10), but they are not directly comparable because of the data preparation and training process are different, the influence of which are significant enough to change the rankings of these NAS methods. We have implemented some of the NAS methods in the framework. More state-of-the-art methods are in progress. There are three advantages of implementing the NAS methods in Auto-Keras. First, it fairly compares the NAS methods independent from other factors (e.g. the choice of optimizer, data augmentation). Second, researchers can easily change the experiment datasets used for NAS. Many of the currently available NAS implementations couple too much with the dataset used, which makes it hard to replace the original dataset with a new one. Third, it saves the effort of finding and running code from different sources. Different code may have different requirements of dependencies and environments, which may conflict with each other. Baseline methods implemented We have implemented four NAS baseline methods: random search : we explore the search space via morphing the network architectures randomly, so the actual performance of the generated neural architecture has no effect on later search. grid search : we manually specified subset of the hyperparameter space to search, i.e., the number of layers and the width of the layers are predefined. greed search : we explore the search space in a greedy way. The \"greedy\" here means the base architecture for the next iteration of search is chosen from those generated by current iteration, the one that have best performance on the training/validation set in our implementation. bayesian optimization : it's the default search strategy of Auto-Keras currently. refer arXiv:1806.10282 for more detail. How to run the baseline methods? Refer to examples/nas/cifar10_tutorial.py for more details. How to implement your own search? To implement your own NAS searcher, you need to implement your own searcher class YOUR_SEARCHER, which is derived from base Searcher class. For your YOUR_SEARCHER class, you must provide implementation of the two abstract method: generate(self, multiprocessing_queue) , which is invoked to generate the next neural architecture. The return value of the generate function should be two elements. The first one is the generated graph. The second one is any other information you want to pass to the update function. If you have multiple values to pass, you need to put them into one tuple. If you don't have any value to pass, you can just put None. update(self, other_info, model_id, graph, metric_value) , which is invoked to update the controller with evaluation result of a neural architecture. The graph and other_info in the parameters are the corresponding return value of the generate function. There is no required return value for this function. You can refer to the default BayesianSearcher as an example. The generate function returns the generated graph and the father ID of the graph in the search tree. Then when the generated model finish training, the father ID ( other_info ) and ID ( model_id ) and instance ( graph ) and metric value ( metric_value ) of the model are passed to update function to update the controller BayesianOptimizer . You can find more example here . You are welcome to implement your own method for NAS in our framework. If it works well, we are happy to merge it into our repo.","title":"Neural Architecture Search"},{"location":"nas/#neural-architecture-search","text":"To help the researchers to do experiments on neural architecture search (NAS), we have implemented several baseline methods using the Auto-Keras framework. The implementations are easy since only the core part of the search algorithm is needed. All other parts of NAS (e.g. data structures for storing neural architectures, training of the neural networks) are done by the Auto-Keras framework.","title":"Neural Architecture Search"},{"location":"nas/#why-implement-nas-papers-in-auto-keras","text":"The NAS papers usually evaluate their work with the same dataset (e.g. CIFAR10), but they are not directly comparable because of the data preparation and training process are different, the influence of which are significant enough to change the rankings of these NAS methods. We have implemented some of the NAS methods in the framework. More state-of-the-art methods are in progress. There are three advantages of implementing the NAS methods in Auto-Keras. First, it fairly compares the NAS methods independent from other factors (e.g. the choice of optimizer, data augmentation). Second, researchers can easily change the experiment datasets used for NAS. Many of the currently available NAS implementations couple too much with the dataset used, which makes it hard to replace the original dataset with a new one. Third, it saves the effort of finding and running code from different sources. Different code may have different requirements of dependencies and environments, which may conflict with each other.","title":"Why implement NAS papers in Auto-Keras?"},{"location":"nas/#baseline-methods-implemented","text":"We have implemented four NAS baseline methods: random search : we explore the search space via morphing the network architectures randomly, so the actual performance of the generated neural architecture has no effect on later search. grid search : we manually specified subset of the hyperparameter space to search, i.e., the number of layers and the width of the layers are predefined. greed search : we explore the search space in a greedy way. The \"greedy\" here means the base architecture for the next iteration of search is chosen from those generated by current iteration, the one that have best performance on the training/validation set in our implementation. bayesian optimization : it's the default search strategy of Auto-Keras currently. refer arXiv:1806.10282 for more detail.","title":"Baseline methods implemented"},{"location":"nas/#how-to-run-the-baseline-methods","text":"Refer to examples/nas/cifar10_tutorial.py for more details.","title":"How to run the baseline methods?"},{"location":"nas/#how-to-implement-your-own-search","text":"To implement your own NAS searcher, you need to implement your own searcher class YOUR_SEARCHER, which is derived from base Searcher class. For your YOUR_SEARCHER class, you must provide implementation of the two abstract method: generate(self, multiprocessing_queue) , which is invoked to generate the next neural architecture. The return value of the generate function should be two elements. The first one is the generated graph. The second one is any other information you want to pass to the update function. If you have multiple values to pass, you need to put them into one tuple. If you don't have any value to pass, you can just put None. update(self, other_info, model_id, graph, metric_value) , which is invoked to update the controller with evaluation result of a neural architecture. The graph and other_info in the parameters are the corresponding return value of the generate function. There is no required return value for this function. You can refer to the default BayesianSearcher as an example. The generate function returns the generated graph and the father ID of the graph in the search tree. Then when the generated model finish training, the father ID ( other_info ) and ID ( model_id ) and instance ( graph ) and metric value ( metric_value ) of the model are passed to update function to update the controller BayesianOptimizer . You can find more example here . You are welcome to implement your own method for NAS in our framework. If it works well, we are happy to merge it into our repo.","title":"How to implement your own search?"},{"location":"start/","text":"Getting Started Installation The installation of Auto-Keras is the same as other python packages. Note: currently, Auto-Keras is only compatible with: Python 3.6 . Latest Stable Version ( pip installation): You can run the following pip installation command in your terminal to install the latest stable version. pip install autokeras Bleeding Edge Version (manual installation): If you want to install the latest development version. You need to download the code from the GitHub repo and run the following commands in the project directory. pip install -r requirements.txt python setup.py install A Simple Example We show an example of image classification on the MNIST dataset, which is a famous benchmark image dataset for hand-written digits classification. Auto-Keras supports different types of data inputs. Data with numpy array (.npy) format. [source] If the images and the labels are already formatted into numpy arrays, you can from keras.datasets import mnist from autokeras.image.image_supervised import ImageClassifier if __name__ == '__main__': (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) clf = ImageClassifier(verbose=True) clf.fit(x_train, y_train, time_limit=12 * 60 * 60) clf.final_fit(x_train, y_train, x_test, y_test, retrain=True) y = clf.evaluate(x_test, y_test) print(y) In the example above, the images and the labels are already formatted into numpy arrays. What if your data are raw image files ( e.g. .jpg, .png, .bmp)? [source] You can use our load_image_dataset function to load the images and their labels as follows. from autokeras.image.image_supervised import load_image_dataset x_train, y_train = load_image_dataset(csv_file_path=\"train/label.csv\", images_path=\"train\") print(x_train.shape) print(y_train.shape) x_test, y_test = load_image_dataset(csv_file_path=\"test/label.csv\", images_path=\"test\") print(x_test.shape) print(y_test.shape) The argument csv_file_path is the path to the CSV file containing the image file names and their corresponding labels. Both csv files and the raw image datasets could be downloaded from link . Here is an example of the csv file. File Name,Label 00000.jpg,5 00001.jpg,0 00002.jpg,4 00003.jpg,1 00004.jpg,9 00005.jpg,2 00006.jpg,1 ... The second argument images_path is the path to the directory containing all the images with those file names listed in the CSV file. The returned values x_train and y_train are the numpy arrays, which can be directly feed into the fit function of ImageClassifier . This CSV file for train or test can be created from folders containing images of a specific class (meaning label): train \u2514\u2500\u2500\u2500class_1 \u2502 \u2502 class_1_image_1.png \u2502 \u2502 class_1_image_2.png | | ... \u2514\u2500\u2500\u2500class_2 \u2502 class_2_image_1.png \u2502 class_2_image_2.png | ... The code below shows an example of how to create the CSV: train_dir = 'train' # Path to the train directory class_dirs = [i for i in os.listdir(path=train_dir) if os.path.isdir(os.path.join(train_dir, i))] with open('train/label.csv', 'w') as train_csv: fieldnames = ['File Name', 'Label'] writer = csv.DictWriter(train_csv, fieldnames=fieldnames) writer.writeheader() label = 0 for current_class in class_dirs: for image in os.listdir(os.path.join(train_dir, current_class)): writer.writerow({'File Name': str(image), 'Label':label}) label += 1 train_csv.close() Enable Multi-GPU Training Auto-Keras support multiple GPU training in the default setting. There's no additional step needed to enable multiple GPU training. However, if multiple-GPU training is not a desirable behavior. You can disable it via environmental variable. CUDA_VISIBLE_DEVICES . For example, in your bash: export CUDA_VISIBLE_DEVICES=0 . Keep in mind that when using multiple-GPU, make sure batch size is big enough that multiple-gpu context switch overhead won't effect the performance too much. Otherwise multiple-gpu training may be slower than single-GPU training. Portable Models How to export Portable model? [source] from autokeras import ImageClassifier clf = ImageClassifier(verbose=True, augment=False) clf.export_autokeras_model(model_file_name) The model will be stored into the path model_file_name . How to load exported Portable model? [source] from autokeras.utils import pickle_from_file model = pickle_from_file(model_file_name) results = model.evaluate(x_test, y_test) print(results) The model will be loaded from the path model_file_name and then you can use the functions listed in PortableImageSupervised . Model Visualizations How to visualize the best selected architecture? [source] While trying to create a model, let's say an Image classifier on MNIST, there is a facility for the user to visualize a .PDF depiction of the best architecture that was chosen by autokeras, after model training is complete. Prerequisites : 1) graphviz must be installed in your system. Refer Installation Guide 2) Additionally, also install \"graphviz\" python package using pip / conda pip: pip install graphviz conda : conda install -c conda-forge python-graphviz If the above installations are complete, proceed with the following steps : Step 1 : Specify a path before starting your model training clf = ImageClassifier(path=\"~/automodels/\",verbose=True, augment=False) # Give a custom path of your choice clf.fit(x_train, y_train, time_limit=30 * 60) clf.final_fit(x_train, y_train, x_test, y_test, retrain=True) Step 2 : After the model training is complete, run examples/visualize.py , whilst passing the same path as parameter if __name__ == '__main__': visualize('~/automodels/') Net Modules MlpModule tutorial. [source] MlpGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with MLP modules Normally, there's two place to call the MlpGenerator, one is call MlpGenerator.fit while the other is MlpGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: mlpModule = MlpModule(loss, metric, searcher_args, path, verbose) Where: * loss and metric determines by the type of training model(classification or regression or others) * search_args can be referred in search.py * path is the path to store the whole searching process and generated model. * verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: mlpModule.fit(n_output_node, input_shape, train_data, test_data, time_limit=24 * 60 * 60) where: * n_output_node: A integer value represent the number of output node in the final layer. * input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). * train_data: A PyTorch DataLoader instance representing the training data. * test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: mlpModule.final_fit(train_data, test_data, trainer_args=None, retrain=False) where: * train_data: A DataLoader instance representing the training data. * test_data: A DataLoader instance representing the testing data. * trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. * retrain: A boolean of whether reinitialize the weights of the model. CnnModule tutorial. [source] CnnGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with basic cnn modules and the ResNet module. Normally, there's two place to call the CnnGenerator, one is call CnnGenerator.fit while the other is CnnGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: from autokeras import CnnModule from autokeras.nn.loss_function import classification_loss from autokeras.nn.metric import Accuracy TEST_FOLDER = \"test\" cnnModule = CnnModule(loss=classification_loss, metric=Accuracy, searcher_args={}, path=TEST_FOLDER, verbose=False) Where: * loss and metric determines by the type of training model(classification or regression or others) * search_args can be referred in search.py * path is the path to store the whole searching process and generated model. * verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: cnnModule.fit(n_output_node, input_shape, train_data, test_data, time_limit=24 * 60 * 60) where: * n_output_node: A integer value represent the number of output node in the final layer. * input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). * train_data: A PyTorch DataLoader instance representing the training data. * test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: cnnModule.final_fit(train_data, test_data, trainer_args=None, retrain=False) where: * train_data: A DataLoader instance representing the training data. * test_data: A DataLoader instance representing the testing data. * trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. * retrain: A boolean of whether reinitialize the weights of the model. Task Modules Automated text classifier tutorial. [source] Class TextClassifier and TextRegressor are designed for automated generate best performance cnn neural architecture for a given text dataset. clf = TextClassifier(verbose=True) clf.fit(x=x_train, y=y_train, time_limit=12 * 60 * 60) x_train: string format text data y_train: int format text label After searching the best model, one can call clf.final_fit to test the best model found in searching. Notes: Preprocessing of the text data: * Class TextClassifier and TextRegressor contains a pre-process of the text data. Which means the input data should be in string format. * The default pre-process model uses the glove6B model from Stanford NLP. * To change the default setting of the pre-process model, one need to change the corresponding variable: EMBEDDING_DIM , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_NAME in constant.py . Pretrained Models Object detection tutorial. [source] by Wuyang Chen from Dr. Atlas Wang's group at CSE Department, Texas A&M. class_id_mapping = {0 : \"Business\", 1 : \"Sci/Tech\", 2 : \"Sports\", 3 : \"World\"} ObjectDetector in object_detector.py is a child class of Pretrained . Currently it can load a pretrained SSD model ( Liu, Wei, et al. \"Ssd: Single shot multibox detector.\" European conference on computer vision. Springer, Cham, 2016. ) and find object(s) in a given image. Let's first import the ObjectDetector and create a detection model ( detector ) with from autokeras.pretrained.object_detector import ObjectDetector detector = ObjectDetector() It will automatically download and load the weights into detector . Note: the ObjectDetector class can automatically detect the existance of available cuda device(s), and use the device if exists. Finally you can make predictions against an image: results = detector.predict(\"/path/to/images/000001.jpg\", output_file_path=\"/path/to/images/\") Function detector.predict() requires the path to the image. If the output_file_path is not given, the detector will just return the numerical results as a list of dictionaries. Each dictionary is like {\"left\": int, \"top\": int, \"width\": int, \"height\": int: \"category\": str, \"confidence\": float}, where left and top is the (left, top) coordinates of the bounding box of the object and width and height are width and height of the box. category is a string representing the class the object belongs to, and the confidence can be regarded as the probability that the model believes its prediction is correct. If the output_file_path is given, then the results mentioned above will be plotted and saved in a new image file with suffix \"_prediction\" into the given output_file_path . If you run the example/object_detection/object_detection_example.py, you will get result [{'category': 'person', 'width': 331, 'height': 500, 'left': 17, 'confidence': 0.9741123914718628, 'top': 0}] Sentiment Analysis tutorial. [source] The sentiment analysis module provides an interface to find the sentiment of any text. The pretrained model is obtained by training Google AI\u2019s BERT model on IMDb dataset . Let\u2019s import the SentimentAnalysis module from text_classifier.py . It is derived from the super class TextClassifier which is the child class of Pretrained class. from autokeras.pretrained.text_classifier import SentimentAnalysis sentiment_analysis = SentimentAnalysis() During initialization of SentimentAnalysis , the pretrained model is loaded into memory i.e. CPU\u2019s or GPU\u2019s, if available. Now, you may directly call the predict function in SentimentAnalysis class on any input sentence provided as a string as shown below. The function returns a value between 0 and 1. polarity = sentiment_cls.predict(\"The model is working well..\") Note: If the output value of the predict function is close to 0, it implies the statement has negative sentiment, whereas value close to 1 implies positive sentiment. If you run sentiment_analysis_example.py , you should get an output value of 0.9 which implies that the input statement The model is working well.. has strong positive sentiment. Topic Classification tutorial. [source] The topic classifier module provides an interface to find the topic of any text. The pretrained model is obtained by training Google AI\u2019s BERT model on AGNews dataset . Let\u2019s import the TopicClassifier module from text_classifier.py . It is derived from the super class TextClassifier which is the child class of Pretrained class. from autokeras.pretrained.text_classifier import TopicClassifier topic_classifier = TopicClassifier() During initialization of TopicClassifier , the pretrained model is loaded into memory i.e. CPU\u2019s or GPU\u2019s, if available. Now, you may directly call the predict function in TopicClassifier class on any input sentence provided as a string as shown below. The function returns one of the fours topics Business , Sci/Tech , World and Sports . class_name = topic_classifier.predict(\"With some more practice, they will definitely make it to finals..\") If you run topic_classifier_example.py , you should see the predict function returns the label Sports , which is the predicted label for the input statement. Voice generator tutorial. [source] The voice generator is a refactor of deepvoice3 . The structure contains three main parts: Encoder : A fully-convolutional encoder, which converts textual features to an internallearned representation. Decoder : A fully-convolutional causal decoder, which decodes the learned representationwith a multi-hop convolutional attention mechanism into a low-dimensional audio repre-sentation (mel-scale spectrograms) in an autoregressive manner. Converter : A fully-convolutional post-processing network, which predicts final vocoderparameters (depending on the vocoder choice) from the decoder hidden states. Unlike thedecoder, the converter is non-causal and can thus depend on future context information For more details, please refer the original paper: Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning Example: from autokeras.pretrained import VoiceGenerator voice_generator = VoiceGenerator() text = \"The approximation of pi is 3.14\" voice_generator.predict(text, \"test.wav\") Voice recognizer tutorial. [source] The voice recognizer is a refactor of deepspeech . The model structure contains two parts: * Encoder: Convolutional layer followed by recurrent neural network and then fully convert network. Output is the hidden voice information. * Decoder: Decode the hidden voice information to the voice wave. For more details, please refer the original paper: Deep Speech 2: End-to-End Speech Recognition in English and Mandarin Because currently torchaudio does not support pip install. So the current package doesn't support audio parsing part. To use the voice recognizer, one should first parse the audio following the standard below: First, install the torchaudio , the install process can refer the repo. Seconder use the following audio parser from autokeras.constant import Constant import torchaudio import scipy.signal import librosa import torch import numpy as np def load_audio(path): sound, _ = torchaudio.load(path) sound = sound.numpy() if len(sound.shape) > 1: if sound.shape[0] == 1: sound = sound.squeeze() else: sound = sound.mean(axis=0) # multiple channels, average return sound class SpectrogramParser: def __init__(self, audio_conf, normalize=False, augment=False): \"\"\" Parses audio file into spectrogram with optional normalization and various augmentations :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds :param normalize(default False): Apply standard mean and deviation normalization to audio tensor :param augment(default False): Apply random tempo and gain perturbations \"\"\" super(SpectrogramParser, self).__init__() self.window_stride = audio_conf['window_stride'] self.window_size = audio_conf['window_size'] self.sample_rate = audio_conf['sample_rate'] self.window = scipy.signal.hamming self.normalize = normalize self.augment = augment self.noise_prob = audio_conf.get('noise_prob') def parse_audio(self, audio_path): y = load_audio(audio_path) n_fft = int(self.sample_rate * self.window_size) win_length = n_fft hop_length = int(self.sample_rate * self.window_stride) # STFT D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=self.window) spect, _ = librosa.magphase(D) # S = log(S+1) spect = np.log1p(spect) spect = torch.FloatTensor(spect) if self.normalize: mean = spect.mean() std = spect.std() spect.add_(-mean) spect.div_(std) return spect parser = SpectrogramParser(Constant.VOICE_RECONGINIZER_AUDIO_CONF, normalize=True) spect = parser.parse_audio(\"test.wav\").contiguous() After this we will have the audio parsed as torch tensor in variable spect . Then we can use the following to recognize the voice: from autokeras.pretrained import VoiceRecognizer voice_recognizer = VoiceRecognizer() print(voice_recognizer.predict(audio_data=spect)) This voice recognizer pretrained model is well tuned based on the AN4 dataset. It has a large probability cannot perform well on other dataset.","title":"Getting Started 0.4"},{"location":"start/#getting-started","text":"","title":"Getting Started"},{"location":"start/#installation","text":"The installation of Auto-Keras is the same as other python packages. Note: currently, Auto-Keras is only compatible with: Python 3.6 .","title":"Installation"},{"location":"start/#latest-stable-version-pip-installation","text":"You can run the following pip installation command in your terminal to install the latest stable version. pip install autokeras","title":"Latest Stable Version (pip installation):"},{"location":"start/#bleeding-edge-version-manual-installation","text":"If you want to install the latest development version. You need to download the code from the GitHub repo and run the following commands in the project directory. pip install -r requirements.txt python setup.py install","title":"Bleeding Edge Version (manual installation):"},{"location":"start/#a-simple-example","text":"We show an example of image classification on the MNIST dataset, which is a famous benchmark image dataset for hand-written digits classification. Auto-Keras supports different types of data inputs.","title":"A Simple Example"},{"location":"start/#data-with-numpy-array-npy-format","text":"[source] If the images and the labels are already formatted into numpy arrays, you can from keras.datasets import mnist from autokeras.image.image_supervised import ImageClassifier if __name__ == '__main__': (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) clf = ImageClassifier(verbose=True) clf.fit(x_train, y_train, time_limit=12 * 60 * 60) clf.final_fit(x_train, y_train, x_test, y_test, retrain=True) y = clf.evaluate(x_test, y_test) print(y) In the example above, the images and the labels are already formatted into numpy arrays.","title":"Data with numpy array (.npy) format."},{"location":"start/#what-if-your-data-are-raw-image-files-eg-jpg-png-bmp","text":"[source] You can use our load_image_dataset function to load the images and their labels as follows. from autokeras.image.image_supervised import load_image_dataset x_train, y_train = load_image_dataset(csv_file_path=\"train/label.csv\", images_path=\"train\") print(x_train.shape) print(y_train.shape) x_test, y_test = load_image_dataset(csv_file_path=\"test/label.csv\", images_path=\"test\") print(x_test.shape) print(y_test.shape) The argument csv_file_path is the path to the CSV file containing the image file names and their corresponding labels. Both csv files and the raw image datasets could be downloaded from link . Here is an example of the csv file. File Name,Label 00000.jpg,5 00001.jpg,0 00002.jpg,4 00003.jpg,1 00004.jpg,9 00005.jpg,2 00006.jpg,1 ... The second argument images_path is the path to the directory containing all the images with those file names listed in the CSV file. The returned values x_train and y_train are the numpy arrays, which can be directly feed into the fit function of ImageClassifier . This CSV file for train or test can be created from folders containing images of a specific class (meaning label): train \u2514\u2500\u2500\u2500class_1 \u2502 \u2502 class_1_image_1.png \u2502 \u2502 class_1_image_2.png | | ... \u2514\u2500\u2500\u2500class_2 \u2502 class_2_image_1.png \u2502 class_2_image_2.png | ... The code below shows an example of how to create the CSV: train_dir = 'train' # Path to the train directory class_dirs = [i for i in os.listdir(path=train_dir) if os.path.isdir(os.path.join(train_dir, i))] with open('train/label.csv', 'w') as train_csv: fieldnames = ['File Name', 'Label'] writer = csv.DictWriter(train_csv, fieldnames=fieldnames) writer.writeheader() label = 0 for current_class in class_dirs: for image in os.listdir(os.path.join(train_dir, current_class)): writer.writerow({'File Name': str(image), 'Label':label}) label += 1 train_csv.close()","title":"What if your data are raw image files (e.g. .jpg, .png, .bmp)?"},{"location":"start/#enable-multi-gpu-training","text":"Auto-Keras support multiple GPU training in the default setting. There's no additional step needed to enable multiple GPU training. However, if multiple-GPU training is not a desirable behavior. You can disable it via environmental variable. CUDA_VISIBLE_DEVICES . For example, in your bash: export CUDA_VISIBLE_DEVICES=0 . Keep in mind that when using multiple-GPU, make sure batch size is big enough that multiple-gpu context switch overhead won't effect the performance too much. Otherwise multiple-gpu training may be slower than single-GPU training.","title":"Enable Multi-GPU Training"},{"location":"start/#portable-models","text":"","title":"Portable Models"},{"location":"start/#how-to-export-portable-model","text":"[source] from autokeras import ImageClassifier clf = ImageClassifier(verbose=True, augment=False) clf.export_autokeras_model(model_file_name) The model will be stored into the path model_file_name .","title":"How to export Portable model?"},{"location":"start/#how-to-load-exported-portable-model","text":"[source] from autokeras.utils import pickle_from_file model = pickle_from_file(model_file_name) results = model.evaluate(x_test, y_test) print(results) The model will be loaded from the path model_file_name and then you can use the functions listed in PortableImageSupervised .","title":"How to load exported Portable model?"},{"location":"start/#model-visualizations","text":"","title":"Model Visualizations"},{"location":"start/#how-to-visualize-the-best-selected-architecture","text":"[source] While trying to create a model, let's say an Image classifier on MNIST, there is a facility for the user to visualize a .PDF depiction of the best architecture that was chosen by autokeras, after model training is complete. Prerequisites : 1) graphviz must be installed in your system. Refer Installation Guide 2) Additionally, also install \"graphviz\" python package using pip / conda pip: pip install graphviz conda : conda install -c conda-forge python-graphviz If the above installations are complete, proceed with the following steps : Step 1 : Specify a path before starting your model training clf = ImageClassifier(path=\"~/automodels/\",verbose=True, augment=False) # Give a custom path of your choice clf.fit(x_train, y_train, time_limit=30 * 60) clf.final_fit(x_train, y_train, x_test, y_test, retrain=True) Step 2 : After the model training is complete, run examples/visualize.py , whilst passing the same path as parameter if __name__ == '__main__': visualize('~/automodels/')","title":"How to visualize the best selected architecture?"},{"location":"start/#net-modules","text":"","title":"Net Modules"},{"location":"start/#mlpmodule-tutorial","text":"[source] MlpGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with MLP modules Normally, there's two place to call the MlpGenerator, one is call MlpGenerator.fit while the other is MlpGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: mlpModule = MlpModule(loss, metric, searcher_args, path, verbose) Where: * loss and metric determines by the type of training model(classification or regression or others) * search_args can be referred in search.py * path is the path to store the whole searching process and generated model. * verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: mlpModule.fit(n_output_node, input_shape, train_data, test_data, time_limit=24 * 60 * 60) where: * n_output_node: A integer value represent the number of output node in the final layer. * input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). * train_data: A PyTorch DataLoader instance representing the training data. * test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: mlpModule.final_fit(train_data, test_data, trainer_args=None, retrain=False) where: * train_data: A DataLoader instance representing the training data. * test_data: A DataLoader instance representing the testing data. * trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. * retrain: A boolean of whether reinitialize the weights of the model.","title":"MlpModule tutorial."},{"location":"start/#cnnmodule-tutorial","text":"[source] CnnGenerator in net_module.py is a child class of Networkmodule . It can generates neural architecture with basic cnn modules and the ResNet module. Normally, there's two place to call the CnnGenerator, one is call CnnGenerator.fit while the other is CnnGenerator.final_fit . For example, in a image classification class ImageClassifier , one can initialize the cnn module as: from autokeras import CnnModule from autokeras.nn.loss_function import classification_loss from autokeras.nn.metric import Accuracy TEST_FOLDER = \"test\" cnnModule = CnnModule(loss=classification_loss, metric=Accuracy, searcher_args={}, path=TEST_FOLDER, verbose=False) Where: * loss and metric determines by the type of training model(classification or regression or others) * search_args can be referred in search.py * path is the path to store the whole searching process and generated model. * verbose is a boolean. Setting it to true prints to stdout. Then, for the searching part, one can call: cnnModule.fit(n_output_node, input_shape, train_data, test_data, time_limit=24 * 60 * 60) where: * n_output_node: A integer value represent the number of output node in the final layer. * input_shape: A tuple to express the shape of every train entry. For example, MNIST dataset would be (28,28,1). * train_data: A PyTorch DataLoader instance representing the training data. * test_data: A PyTorch DataLoader instance representing the testing data. * time_limit: A integer value represents the time limit on searching for models. And for final testing(testing the best searched model), one can call: cnnModule.final_fit(train_data, test_data, trainer_args=None, retrain=False) where: * train_data: A DataLoader instance representing the training data. * test_data: A DataLoader instance representing the testing data. * trainer_args: A dictionary containing the parameters of the ModelTrainer constructor. * retrain: A boolean of whether reinitialize the weights of the model.","title":"CnnModule tutorial."},{"location":"start/#task-modules","text":"","title":"Task Modules"},{"location":"start/#automated-text-classifier-tutorial","text":"[source] Class TextClassifier and TextRegressor are designed for automated generate best performance cnn neural architecture for a given text dataset. clf = TextClassifier(verbose=True) clf.fit(x=x_train, y=y_train, time_limit=12 * 60 * 60) x_train: string format text data y_train: int format text label After searching the best model, one can call clf.final_fit to test the best model found in searching. Notes: Preprocessing of the text data: * Class TextClassifier and TextRegressor contains a pre-process of the text data. Which means the input data should be in string format. * The default pre-process model uses the glove6B model from Stanford NLP. * To change the default setting of the pre-process model, one need to change the corresponding variable: EMBEDDING_DIM , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_LINK , PRE_TRAIN_FILE_NAME in constant.py .","title":"Automated text classifier tutorial."},{"location":"start/#pretrained-models","text":"","title":"Pretrained Models"},{"location":"start/#object-detection-tutorial","text":"[source]","title":"Object detection tutorial."},{"location":"start/#by-wuyang-chen-from-dr-atlas-wangs-group-at-cse-department-texas-am","text":"class_id_mapping = {0 : \"Business\", 1 : \"Sci/Tech\", 2 : \"Sports\", 3 : \"World\"} ObjectDetector in object_detector.py is a child class of Pretrained . Currently it can load a pretrained SSD model ( Liu, Wei, et al. \"Ssd: Single shot multibox detector.\" European conference on computer vision. Springer, Cham, 2016. ) and find object(s) in a given image. Let's first import the ObjectDetector and create a detection model ( detector ) with from autokeras.pretrained.object_detector import ObjectDetector detector = ObjectDetector() It will automatically download and load the weights into detector . Note: the ObjectDetector class can automatically detect the existance of available cuda device(s), and use the device if exists. Finally you can make predictions against an image: results = detector.predict(\"/path/to/images/000001.jpg\", output_file_path=\"/path/to/images/\") Function detector.predict() requires the path to the image. If the output_file_path is not given, the detector will just return the numerical results as a list of dictionaries. Each dictionary is like {\"left\": int, \"top\": int, \"width\": int, \"height\": int: \"category\": str, \"confidence\": float}, where left and top is the (left, top) coordinates of the bounding box of the object and width and height are width and height of the box. category is a string representing the class the object belongs to, and the confidence can be regarded as the probability that the model believes its prediction is correct. If the output_file_path is given, then the results mentioned above will be plotted and saved in a new image file with suffix \"_prediction\" into the given output_file_path . If you run the example/object_detection/object_detection_example.py, you will get result [{'category': 'person', 'width': 331, 'height': 500, 'left': 17, 'confidence': 0.9741123914718628, 'top': 0}]","title":"by Wuyang Chen from Dr. Atlas Wang's group at CSE Department, Texas A&amp;M."},{"location":"start/#sentiment-analysis-tutorial","text":"[source] The sentiment analysis module provides an interface to find the sentiment of any text. The pretrained model is obtained by training Google AI\u2019s BERT model on IMDb dataset . Let\u2019s import the SentimentAnalysis module from text_classifier.py . It is derived from the super class TextClassifier which is the child class of Pretrained class. from autokeras.pretrained.text_classifier import SentimentAnalysis sentiment_analysis = SentimentAnalysis() During initialization of SentimentAnalysis , the pretrained model is loaded into memory i.e. CPU\u2019s or GPU\u2019s, if available. Now, you may directly call the predict function in SentimentAnalysis class on any input sentence provided as a string as shown below. The function returns a value between 0 and 1. polarity = sentiment_cls.predict(\"The model is working well..\") Note: If the output value of the predict function is close to 0, it implies the statement has negative sentiment, whereas value close to 1 implies positive sentiment. If you run sentiment_analysis_example.py , you should get an output value of 0.9 which implies that the input statement The model is working well.. has strong positive sentiment.","title":"Sentiment Analysis tutorial."},{"location":"start/#topic-classification-tutorial","text":"[source] The topic classifier module provides an interface to find the topic of any text. The pretrained model is obtained by training Google AI\u2019s BERT model on AGNews dataset . Let\u2019s import the TopicClassifier module from text_classifier.py . It is derived from the super class TextClassifier which is the child class of Pretrained class. from autokeras.pretrained.text_classifier import TopicClassifier topic_classifier = TopicClassifier() During initialization of TopicClassifier , the pretrained model is loaded into memory i.e. CPU\u2019s or GPU\u2019s, if available. Now, you may directly call the predict function in TopicClassifier class on any input sentence provided as a string as shown below. The function returns one of the fours topics Business , Sci/Tech , World and Sports . class_name = topic_classifier.predict(\"With some more practice, they will definitely make it to finals..\") If you run topic_classifier_example.py , you should see the predict function returns the label Sports , which is the predicted label for the input statement.","title":"Topic Classification tutorial."},{"location":"start/#voice-generator-tutorial","text":"[source] The voice generator is a refactor of deepvoice3 . The structure contains three main parts: Encoder : A fully-convolutional encoder, which converts textual features to an internallearned representation. Decoder : A fully-convolutional causal decoder, which decodes the learned representationwith a multi-hop convolutional attention mechanism into a low-dimensional audio repre-sentation (mel-scale spectrograms) in an autoregressive manner. Converter : A fully-convolutional post-processing network, which predicts final vocoderparameters (depending on the vocoder choice) from the decoder hidden states. Unlike thedecoder, the converter is non-causal and can thus depend on future context information For more details, please refer the original paper: Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence Learning Example: from autokeras.pretrained import VoiceGenerator voice_generator = VoiceGenerator() text = \"The approximation of pi is 3.14\" voice_generator.predict(text, \"test.wav\")","title":"Voice generator tutorial."},{"location":"start/#voice-recognizer-tutorial","text":"[source] The voice recognizer is a refactor of deepspeech . The model structure contains two parts: * Encoder: Convolutional layer followed by recurrent neural network and then fully convert network. Output is the hidden voice information. * Decoder: Decode the hidden voice information to the voice wave. For more details, please refer the original paper: Deep Speech 2: End-to-End Speech Recognition in English and Mandarin Because currently torchaudio does not support pip install. So the current package doesn't support audio parsing part. To use the voice recognizer, one should first parse the audio following the standard below: First, install the torchaudio , the install process can refer the repo. Seconder use the following audio parser from autokeras.constant import Constant import torchaudio import scipy.signal import librosa import torch import numpy as np def load_audio(path): sound, _ = torchaudio.load(path) sound = sound.numpy() if len(sound.shape) > 1: if sound.shape[0] == 1: sound = sound.squeeze() else: sound = sound.mean(axis=0) # multiple channels, average return sound class SpectrogramParser: def __init__(self, audio_conf, normalize=False, augment=False): \"\"\" Parses audio file into spectrogram with optional normalization and various augmentations :param audio_conf: Dictionary containing the sample rate, window and the window length/stride in seconds :param normalize(default False): Apply standard mean and deviation normalization to audio tensor :param augment(default False): Apply random tempo and gain perturbations \"\"\" super(SpectrogramParser, self).__init__() self.window_stride = audio_conf['window_stride'] self.window_size = audio_conf['window_size'] self.sample_rate = audio_conf['sample_rate'] self.window = scipy.signal.hamming self.normalize = normalize self.augment = augment self.noise_prob = audio_conf.get('noise_prob') def parse_audio(self, audio_path): y = load_audio(audio_path) n_fft = int(self.sample_rate * self.window_size) win_length = n_fft hop_length = int(self.sample_rate * self.window_stride) # STFT D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length, win_length=win_length, window=self.window) spect, _ = librosa.magphase(D) # S = log(S+1) spect = np.log1p(spect) spect = torch.FloatTensor(spect) if self.normalize: mean = spect.mean() std = spect.std() spect.add_(-mean) spect.div_(std) return spect parser = SpectrogramParser(Constant.VOICE_RECONGINIZER_AUDIO_CONF, normalize=True) spect = parser.parse_audio(\"test.wav\").contiguous() After this we will have the audio parsed as torch tensor in variable spect . Then we can use the following to recognize the voice: from autokeras.pretrained import VoiceRecognizer voice_recognizer = VoiceRecognizer() print(voice_recognizer.predict(audio_data=spect)) This voice recognizer pretrained model is well tuned based on the AN4 dataset. It has a large probability cannot perform well on other dataset.","title":"Voice recognizer tutorial."},{"location":"task/","text":"Task API AutoKeras support the following task APIs. [source] ImageClassifier autokeras.task.ImageClassifier(num_classes=None, multi_label=False, loss=None, metrics=None, name='image_classifier', max_trials=100, directory=None, seed=None) AutoKeras image classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'image_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed. [source] ImageRegressor autokeras.task.ImageRegressor(output_dim=None, loss=None, metrics=None, name='image_regressor', max_trials=100, directory=None, seed=None) AutoKeras image regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'image_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed. [source] TextClassifier autokeras.task.TextClassifier(num_classes=None, multi_label=False, loss=None, metrics=None, name='text_classifier', max_trials=100, directory=None, seed=None) AutoKeras text classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'text_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed. [source] TextRegressor autokeras.task.TextRegressor(output_dim=None, loss=None, metrics=None, name='text_regressor', max_trials=100, directory=None, seed=None) AutoKeras text regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'text_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed. Coming Soon: StructuredDataClassifier StructuredDataRegressor TimeSeriesForecaster","title":"Task API"},{"location":"task/#task-api","text":"AutoKeras support the following task APIs. [source]","title":"Task API"},{"location":"task/#imageclassifier","text":"autokeras.task.ImageClassifier(num_classes=None, multi_label=False, loss=None, metrics=None, name='image_classifier', max_trials=100, directory=None, seed=None) AutoKeras image classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'image_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed. [source]","title":"ImageClassifier"},{"location":"task/#imageregressor","text":"autokeras.task.ImageRegressor(output_dim=None, loss=None, metrics=None, name='image_regressor', max_trials=100, directory=None, seed=None) AutoKeras image regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'image_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed. [source]","title":"ImageRegressor"},{"location":"task/#textclassifier","text":"autokeras.task.TextClassifier(num_classes=None, multi_label=False, loss=None, metrics=None, name='text_classifier', max_trials=100, directory=None, seed=None) AutoKeras text classification class. Arguments num_classes : Int. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'binary_crossentropy' or 'categorical_crossentropy' based on the number of classes. metrics : A list of Keras metrics. Defaults to use 'accuracy'. name : String. The name of the AutoModel. Defaults to 'text_classifier'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed. [source]","title":"TextClassifier"},{"location":"task/#textregressor","text":"autokeras.task.TextRegressor(output_dim=None, loss=None, metrics=None, name='text_regressor', max_trials=100, directory=None, seed=None) AutoKeras text regression class. Arguments output_dim : Int. The number of output dimensions. Defaults to None. If None, it will infer from the data. multi_label : Boolean. Defaults to False. loss : A Keras loss function. Defaults to use 'mean_squared_error'. metrics : A list of Keras metrics. Defaults to use 'mean_squared_error'. name : String. The name of the AutoModel. Defaults to 'text_regressor'. max_trials : Int. The maximum number of different Keras Models to try. The search may finish before reaching the max_trials. Defaults to 100. directory : String. The path to a directory for storing the search outputs. Defaults to None, which would create a folder with the name of the AutoModel in the current directory. seed : Int. Random seed.","title":"TextRegressor"},{"location":"task/#coming-soon","text":"StructuredDataClassifier StructuredDataRegressor TimeSeriesForecaster","title":"Coming Soon:"},{"location":"tutorial/","text":"AutoKeras 1.0 Tutorial In AutoKeras, there are 3 levels of APIs: task API, IO API, and functional API. Task API We have designed an extremely simple interface for a series of tasks. The following code example shows how to do image classification with the task API. import autokeras as ak from keras.datasets import mnist # Prepare the data. (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) # Search and train the classifier. clf = ak.ImageClassifier(max_trials=100) clf.fit(x_train, y_train) y = clf.predict(x_test, y_test) See the documentation of Task API for more details. IO API The following code example shows how to use IO API for multi-modal and multi-task scenarios using AutoModel import numpy as np import autokeras as ak from keras.datasets import mnist # Prepare the data. (x_train, y_classification), (x_test, y_test) = mnist.load_data() x_image = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) x_structured = np.random.rand(x_train.shape[0], 100) y_regression = np.random.rand(x_train.shape[0], 1) # Build model and train. automodel = ak.AutoModel( inputs=[ak.ImageInput(), ak.StructuredDataInput()], outputs=[ak.RegressionHead(metrics=['mae']), ak.ClassificationHead(loss='categorical_crossentropy', metrics=['accuracy'])]) automodel.fit([x_image, x_structured], [y_regression, y_classification]) Now we support ImageInput , TextInput , and StructuredDataInput . Functional API You can also define your own neural architecture with the predefined blocks and GraphAutoModel . import autokeras as ak import numpy as np import tensorflow as tf from keras.datasets import mnist # Prepare the data. (x_train, y_classification), (x_test, y_test) = mnist.load_data() x_image = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) x_structured = np.random.rand(x_train.shape[0], 100) y_regression = np.random.rand(x_train.shape[0], 1) # Build model and train. inputs = ak.ImageInput(shape=(28, 28, 1)) outputs1 = ak.ResNetBlock(version='next')(inputs) outputs2 = ak.XceptionBlock()(inputs) image_outputs = ak.Merge()((outputs1, outputs2)) structured_inputs = ak.StructuredInput() structured_outputs = ak.DenseBlock()(structured_inputs) merged_outputs = ak.Merge()((image_outputs, structured_outputs)) classification_outputs = ak.ClassificationHead()(merged_outputs) regression_outputs = ak.RegressionHead()(merged_outputs) automodel = ak.GraphAutoModel(inputs=inputs, outputs=[regression_outputs, classification_outputs]) automodel.fit((x_image, x_structured), (y_regression, y_classification), trials=100, epochs=200, callbacks=[tf.keras.callbacks.EarlyStopping(), tf.keras.callbacks.LearningRateScheduler()]) For complete list of blocks, please checkout the documentation here .","title":"Getting Started 1.0"},{"location":"tutorial/#autokeras-10-tutorial","text":"In AutoKeras, there are 3 levels of APIs: task API, IO API, and functional API.","title":"AutoKeras 1.0 Tutorial"},{"location":"tutorial/#task-api","text":"We have designed an extremely simple interface for a series of tasks. The following code example shows how to do image classification with the task API. import autokeras as ak from keras.datasets import mnist # Prepare the data. (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) # Search and train the classifier. clf = ak.ImageClassifier(max_trials=100) clf.fit(x_train, y_train) y = clf.predict(x_test, y_test) See the documentation of Task API for more details.","title":"Task API"},{"location":"tutorial/#io-api","text":"The following code example shows how to use IO API for multi-modal and multi-task scenarios using AutoModel import numpy as np import autokeras as ak from keras.datasets import mnist # Prepare the data. (x_train, y_classification), (x_test, y_test) = mnist.load_data() x_image = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) x_structured = np.random.rand(x_train.shape[0], 100) y_regression = np.random.rand(x_train.shape[0], 1) # Build model and train. automodel = ak.AutoModel( inputs=[ak.ImageInput(), ak.StructuredDataInput()], outputs=[ak.RegressionHead(metrics=['mae']), ak.ClassificationHead(loss='categorical_crossentropy', metrics=['accuracy'])]) automodel.fit([x_image, x_structured], [y_regression, y_classification]) Now we support ImageInput , TextInput , and StructuredDataInput .","title":"IO API"},{"location":"tutorial/#functional-api","text":"You can also define your own neural architecture with the predefined blocks and GraphAutoModel . import autokeras as ak import numpy as np import tensorflow as tf from keras.datasets import mnist # Prepare the data. (x_train, y_classification), (x_test, y_test) = mnist.load_data() x_image = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) x_structured = np.random.rand(x_train.shape[0], 100) y_regression = np.random.rand(x_train.shape[0], 1) # Build model and train. inputs = ak.ImageInput(shape=(28, 28, 1)) outputs1 = ak.ResNetBlock(version='next')(inputs) outputs2 = ak.XceptionBlock()(inputs) image_outputs = ak.Merge()((outputs1, outputs2)) structured_inputs = ak.StructuredInput() structured_outputs = ak.DenseBlock()(structured_inputs) merged_outputs = ak.Merge()((image_outputs, structured_outputs)) classification_outputs = ak.ClassificationHead()(merged_outputs) regression_outputs = ak.RegressionHead()(merged_outputs) automodel = ak.GraphAutoModel(inputs=inputs, outputs=[regression_outputs, classification_outputs]) automodel.fit((x_image, x_structured), (y_regression, y_classification), trials=100, epochs=200, callbacks=[tf.keras.callbacks.EarlyStopping(), tf.keras.callbacks.LearningRateScheduler()]) For complete list of blocks, please checkout the documentation here .","title":"Functional API"},{"location":"examples/functional_api/","text":"import numpy as np import tensorflow as tf from keras.datasets import mnist # Prepare the data. (x_train, y_classification), (x_test, y_test) = mnist.load_data() data_slice = 5 x_train = x_train[:data_slice] y_classification = y_classification[:data_slice] x_test = x_test[:data_slice] y_test = y_test[:data_slice] x_image = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) x_structured = np.random.rand(x_train.shape[0], 100) y_regression = np.random.rand(x_train.shape[0], 1) y_classification = y_classification.reshape(-1, 1) # Build model and train. inputs = ak.ImageInput(shape=(28, 28, 1)) outputs1 = ak.ResNetBlock(version='next')(inputs) outputs2 = ak.XceptionBlock()(inputs) image_outputs = ak.Merge()((outputs1, outputs2)) structured_inputs = ak.StructuredDataInput() structured_outputs = ak.DenseBlock()(structured_inputs) merged_outputs = ak.Merge()((structured_outputs, image_outputs)) classification_outputs = ak.ClassificationHead()(merged_outputs) regression_outputs = ak.RegressionHead()(merged_outputs) automodel = ak.GraphAutoModel(inputs=[inputs, structured_inputs], outputs=[regression_outputs, classification_outputs]) automodel.fit((x_image, x_structured), (y_regression, y_classification), # trials=100, validation_split=0.2, epochs=200, callbacks=[tf.keras.callbacks.EarlyStopping()])","title":"Functional api"},{"location":"examples/io_api/","text":"import autokeras as ak import tensorflow as tf from keras.datasets import mnist # Prepare the data. (x_train, y_classification), (x_test, y_test) = mnist.load_data() data_slice = 20 x_train = x_train[:data_slice] print(x_train.dtype) y_classification = y_classification[:data_slice] x_test = x_test[:data_slice] y_test = y_test[:data_slice] x_train = x_train.astype(np.float64) x_test = x_test.astype(np.float64) print(x_train.dtype) # x_image = np.reshape(x_train, (200, 28, 28, 1)) # x_test = np.reshape(x_test, (200, 28, 28, 1)) x_image = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) '''x_structured = np.random.rand(x_train.shape[0], 100) y_regression = np.random.rand(x_train.shape[0], 1)''' x_structured = np.random.rand(x_train.shape[0], 100) y_regression = np.random.rand(x_train.shape[0], 1) y_classification = y_classification.reshape(-1, 1) # y_classification = np.reshape(y_classification, (-1, 1)) # Build model and train. automodel = ak.AutoModel( inputs=[ak.ImageInput(), ak.StructuredDataInput()], outputs=[ak.RegressionHead(metrics=['mae']), ak.ClassificationHead(loss='categorical_crossentropy', metrics=['accuracy'])]) automodel.fit([x_image, x_structured], [y_regression, y_classification], validation_split=0.2)","title":"Io api"},{"location":"examples/task_api/","text":"from tensorflow.python.keras.datasets import mnist, cifar10 import numpy as np # Prepare the data. (x_train, y_train), (x_test, y_test) = mnist.load_data() data_slice = 50 x_train = x_train[:data_slice] y_train = y_train[:data_slice] x_test = x_test[:data_slice] y_test = y_test[:data_slice] x_train = x_train.astype(np.float64) x_test = x_test.astype(np.float64) if len(np.shape(x_train)) == 3: # If the raw image has 'Channel', we don't have to add one. x_train = x_train.reshape(x_train.shape + (1,)) x_test = x_test.reshape(x_test.shape + (1,)) # Search and train the classifier. clf = ak.ImageClassifier(max_trials=3) clf.fit(x_train, y_train, validation_data=(x_test, y_test)) y = clf.predict(x_test, y_test)","title":"Task api"}]}